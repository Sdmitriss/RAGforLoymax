В проекте реализована архитектура Retrieval-Augmented Generation (RAG),обеспечивающая расширение ответов модели за счёт поиска информации в локальной базе данных.

RAG) реализован с использованием следующих компонентов:
- Векторное хранилище: Faiss
- Модель эмбеддингов: cointegrated/LaBSE-en-ru
- Основная LLM: YandexGPT-5-Lite-8B-instruct-Q4_K_M (guff)
- API-интерфейс: FastAPI

Модели и параметры задаются через конфигурационный файл.

Структура  проекта:

```
project_root/
├── config/
│   └── config.json   файл  конфигурации, 
├── data_raw/
│   └── ... (исходные JSON данные) 
├── data/
│   └── ... ( опционно сохранённые CSV и pickle данные  в форме  DataFrame)
├── logs/
│   └── ... (логи)  error.logs    info.logs( только INFO, WARNING  )
├── src/
│   ├── __init__.py
│   ├── custom_logging.py
│   ├── preprocessing.py
│   ├── faiss_service.py
|   └── llm_answer.ru
├── .cashe/
│   ├── faiss/
|   | └── (файлы инициализации индекса) index.file.index  text.index.pkl (связанные  тексты)
|   └── (кэш моделей)
├── main_indexer.py 
├── main_answer.py
├── requirements_indexer.txt   пакеты окружения для main_indexer
├── requirements_answer.txt    пакеты  окружения для  main_answer
├── Dockerfile.indexer
├── Dockerfile.answer
├── docker-compose.yml
├── requests_example.ipynb
├── edit_config_json.ipynb
└── README.md
```
Краткое  описание  фуункционала  модулей(подробнее в докстрингах):<br>
- custom_logger.py -  пишет логи в  файлы error.logs ,info.logs(( только INFO, WARNING  ),есть отдельные методы для вывода в консоль;
- preprocessing.py - по словарю json  создает DF, последовательно обрабатывает пропуски и дубликаты, удаляет тексты с количеством слов ниже порога; основные методы: clean() — возвращает очищенный DataFrame, list_texts() — список текстов; опционально сохраняет DataFrame в папку data;
- faiss_service faiss_service реализует логику работы векторного хранилища: создание индекса (IndexFlatL2), инициализацию эмбеддингами текстов, добавление эмбеддингов, возвращение похожих текстов и расстояний, сохранение хранилища, загрузку сохранённых данных и очистку для повторной инициализации,  возвращает похожие тексты  и расстояния. Парметры настраиваются в config.json;
- llm_answer принимает пользовательский запрос, обращается к faiss_service, анализирует полученные расстояния и тексты, формирует на их основе prompt для LLM и возвращает сгенерированный текстовый ответ.

Основные скрипты:

- main_faiss.py — FastAPI-сервис на порту 8000. Инициализирует модель эмбеддингов, создаёт или загружает индекс, предоставляет     эндпоинты для добавления текстов, поиска, удаления и переинициализации хранилища.

- main_answer.py — FastAPI-сервис на порту 8001. Принимает запрос пользователя, обращается к faiss_service, формирует prompt для LLM на основе найденных текстов и возвращает сгенерированный ответ.


requests_example.ipnb - содержит подробную инструкцию  по работе  сэндпоинтами  описаниебпримеры запросов

edit_config_json.ipnb - содержит описание  парметров конфигурации, с возможностью их редактирования.

config.json{}

- file_name_texts — имя файла для сохранения списка текстов, связанных с индексом;
- file_name_texts_add — имя файла для сохранения списка  добавленных текстов, связанных с индексом;
- file_name_index — имя файла FAISS-индекса;
- name_json_init — имя JSON-файла с исходными данными для инициализации индекса;
- folder_model — директория для хранения моделей и кэша;
- model_embed_name — название модели эмбеддингов (SentenceTransformer);
- model_llm_name — параметры основной LLM-модели {'repo_id':репозиторий , 'filnename': имя файла GGUF};
- config_LLM — настройки генерации текста: максимальное число токенов, стоп-символы, температура, top-p;
- min_words — минимальное количество слов в тексте для включения в индекс( для модуля preprocessing.py);
- top_k_faiss — количество ближайших соседей, возвращаемых FAISS;
- threshold — порог расстояния для включения текста в ответ;
- distance_diff_vector — минимальная разница между расстояниями, чтобы учитывать в выборке.

